{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib"
      ],
      "metadata": {
        "id": "Lal-9nqCxoAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AjlzBSk0W0hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_columns(df: pd.DataFrame):\n",
        "  required = ['catalog_content_clean', 'image_name', 'price_log']\n",
        "  for c in required:\n",
        "    if c not in df.columns:\n",
        "     raise ValueError(f\"Required column '{c}' not found in CSV\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_placeholder_image(size=(224, 224)) -> Image.Image:\n",
        "# black placeholder for missing images\n",
        "  return Image.new('RGB', size, (0, 0, 0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_image(image_path: str, resize_to=(224, 224)) -> Image.Image:\n",
        "  try:\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    # deterministic resize to (256,256) as requested\n",
        "    img = img.resize(resize_to, resample=Image.BICUBIC)\n",
        "    return img\n",
        "  except Exception:\n",
        "    return make_placeholder_image(size=resize_to)\n",
        "\n"
      ],
      "metadata": {
        "id": "81BFy2kZxo4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "B0IiC1rjTCSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class ProductDataset(Dataset):\n",
        "    \"\"\"Dataset for image-text pairs from e-commerce products.\"\"\"\n",
        "\n",
        "    def __init__(self, df, images_dir, processor, resize_to=(224, 224)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.images_dir = images_dir\n",
        "        self.processor = processor\n",
        "        self.resize_to = resize_to\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = str(row['item_name']) if pd.notna(row['item_name']) else \"\"\n",
        "\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.images_dir, str(row['image_name']))\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image = image.resize(self.resize_to, resample=Image.BICUBIC)\n",
        "        except Exception:\n",
        "            image = Image.new('RGB', self.resize_to, (0, 0, 0))\n",
        "\n",
        "        # Process inputs\n",
        "        inputs = self.processor(\n",
        "            text=[text],\n",
        "            images=image,\n",
        "            return_tensors='pt',\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        # Remove batch dimension\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'pixel_values': inputs['pixel_values'].squeeze(0)\n",
        "        }"
      ],
      "metadata": {
        "id": "K7hGsl252vQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_clip(df: pd.DataFrame,\n",
        "                  images_dir: str,\n",
        "                  model_name: str = 'openai/clip-vit-base-patch32',\n",
        "                  output_dir: str = 'finetuned_clip',\n",
        "                  batch_size: int = 32,\n",
        "                  num_epochs: int = 5,\n",
        "                  learning_rate: float = 5e-6,\n",
        "                  train_split: float = 0.9,\n",
        "                  temperature: float = 0.07,\n",
        "                  device: str = None):\n",
        "    \"\"\"\n",
        "    Fine-tune CLIP model on e-commerce product images and descriptions.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with 'catalog_content_clean' and 'image_name' columns\n",
        "        images_dir: Directory containing product images\n",
        "        model_name: Pretrained CLIP model name\n",
        "        output_dir: Where to save the fine-tuned model\n",
        "        batch_size: Training batch size\n",
        "        num_epochs: Number of training epochs\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        train_split: Fraction of data to use for training (rest for validation)\n",
        "        temperature: Temperature parameter for contrastive loss\n",
        "        device: Device to use ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        Fine-tuned model\n",
        "    \"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model and processor\n",
        "    print(\"Loading pretrained CLIP model...\")\n",
        "    model = CLIPModel.from_pretrained(model_name)\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "\n",
        "    # Split data\n",
        "    n = len(df)\n",
        "    n_train = int(n * train_split)\n",
        "    train_df = df.iloc[:n_train]\n",
        "    val_df = df.iloc[n_train:]\n",
        "\n",
        "    print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = ProductDataset(train_df, images_dir, processor)\n",
        "    val_dataset = ProductDataset(val_df, images_dir, processor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Setup optimizer (only fine-tune vision and text encoders)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device, temperature)\n",
        "        val_loss = validate(model, val_loader, device, temperature)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            model.save_pretrained(output_dir)\n",
        "            processor.save_pretrained(output_dir)\n",
        "            print(f\"Saved best model to {output_dir}\")\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "5Cp2VmjE22wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_embeddings_with_finetuned_model(df: pd.DataFrame,\n",
        "                                            images_dir: str,\n",
        "                                            model_path: str,\n",
        "                                            batch_size: int = 32,\n",
        "                                            device: str = None,\n",
        "                                            out_dir: str = 'outputs_finetuned'):\n",
        "    \"\"\"\n",
        "    Compute embeddings using the fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with product data\n",
        "        images_dir: Directory containing images\n",
        "        model_path: Path to fine-tuned model\n",
        "        batch_size: Batch size for processing\n",
        "        device: Device to use\n",
        "        out_dir: Output directory for embeddings\n",
        "\n",
        "    Returns:\n",
        "        text_embeddings, image_embeddings\n",
        "    \"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load fine-tuned model\n",
        "    print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "    model = CLIPModel.from_pretrained(model_path)\n",
        "    processor = CLIPProcessor.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    def load_image_safe(path: str, resize_to=(224, 224)):\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img = img.resize(resize_to, resample=Image.BICUBIC)\n",
        "            return img\n",
        "        except Exception:\n",
        "            return Image.new('RGB', resize_to, (0, 0, 0))\n",
        "\n",
        "    text_embs = []\n",
        "    image_embs = []\n",
        "\n",
        "    for idx in tqdm(range(0, len(df), batch_size), desc=\"Computing embeddings\"):\n",
        "        end = min(len(df), idx + batch_size)\n",
        "        batch = df.iloc[idx:end]\n",
        "\n",
        "        texts = batch['catalog_content_clean'].fillna('').tolist()\n",
        "        images = [load_image_safe(os.path.join(images_dir, str(fn)))\n",
        "                 for fn in batch['image_name'].tolist()]\n",
        "\n",
        "        inputs = processor(text=texts, images=images, return_tensors='pt',\n",
        "                          padding=True, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        t_emb = outputs.text_embeds.detach().cpu().numpy()\n",
        "        i_emb = outputs.image_embeds.detach().cpu().numpy()\n",
        "\n",
        "        # L2 normalize\n",
        "        t_emb = t_emb / (np.linalg.norm(t_emb, axis=1, keepdims=True) + 1e-12)\n",
        "        i_emb = i_emb / (np.linalg.norm(i_emb, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "        text_embs.append(t_emb)\n",
        "        image_embs.append(i_emb)\n",
        "\n",
        "    text_embeddings = np.vstack(text_embs)\n",
        "    image_embeddings = np.vstack(image_embs)\n",
        "\n",
        "    # Save embeddings\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    np.save(os.path.join(out_dir, 'text_embeddings_finetuned.npy'), text_embeddings)\n",
        "    np.save(os.path.join(out_dir, 'image_embeddings_finetuned.npy'), image_embeddings)\n",
        "\n",
        "    print(f\"Saved embeddings. Shapes: {text_embeddings.shape}, {image_embeddings.shape}\")\n",
        "    return text_embeddings, image_embeddings"
      ],
      "metadata": {
        "id": "h9PDOe7h22y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('catalog_25k.csv')"
      ],
      "metadata": {
        "id": "0LHJ8BI03OjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/content/drive/MyDrive/images_train\""
      ],
      "metadata": {
        "id": "uJyo3jHy3OmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = finetune_clip(\n",
        "    df=df,\n",
        "    images_dir=images_dir,\n",
        "    num_epochs=5,\n",
        "    batch_size=32,\n",
        "    learning_rate=5e-6,\n",
        "    output_dir='/content/fine_tune_model'\n",
        ")"
      ],
      "metadata": {
        "id": "Ip5RA4Nm3Opd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8dXHKRWvWqDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embs, img_embs = compute_embeddings_with_finetuned_model(\n",
        "    df=df,\n",
        "    images_dir=images_dir,\n",
        "    model_path='/content/drive/MyDrive/finetuned_clip'\n",
        ")"
      ],
      "metadata": {
        "id": "9gaIQTNn222h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd304d8e-8c7d-44e9-9b95-8e424c4531b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading fine-tuned model from /content/drive/MyDrive/finetuned_clip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Computing embeddings:   0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\n",
            "Computing embeddings:   0%|          | 1/782 [00:01<15:59,  1.23s/it]\u001b[A\n",
            "Computing embeddings:   0%|          | 2/782 [00:02<17:38,  1.36s/it]\u001b[A\n",
            "Computing embeddings:   0%|          | 3/782 [00:05<23:26,  1.81s/it]\u001b[A\n",
            "Computing embeddings:   1%|          | 4/782 [00:06<23:12,  1.79s/it]\u001b[A\n",
            "Computing embeddings:   1%|          | 5/782 [00:08<24:13,  1.87s/it]\u001b[A\n",
            "Computing embeddings:   1%|          | 6/782 [00:10<22:35,  1.75s/it]\u001b[A\n",
            "Computing embeddings:   1%|          | 7/782 [00:12<22:50,  1.77s/it]\u001b[A\n",
            "Computing embeddings:   1%|          | 8/782 [00:14<23:32,  1.83s/it]\u001b[A\n",
            "Computing embeddings:   1%|          | 9/782 [00:15<21:51,  1.70s/it]\u001b[A\n",
            "Computing embeddings:   1%|▏         | 10/782 [00:17<23:11,  1.80s/it]\u001b[A\n",
            "Computing embeddings:   1%|▏         | 11/782 [00:19<25:31,  1.99s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 12/782 [00:21<23:39,  1.84s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 13/782 [00:22<21:40,  1.69s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 14/782 [00:24<20:57,  1.64s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 15/782 [00:26<21:22,  1.67s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 16/782 [00:27<22:24,  1.76s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 17/782 [00:30<24:14,  1.90s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 18/782 [00:33<29:25,  2.31s/it]\u001b[A\n",
            "Computing embeddings:   2%|▏         | 19/782 [00:35<26:34,  2.09s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 20/782 [00:36<24:24,  1.92s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 21/782 [00:37<22:01,  1.74s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 22/782 [00:39<21:13,  1.68s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 23/782 [00:41<21:48,  1.72s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 24/782 [00:43<22:19,  1.77s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 25/782 [00:45<24:02,  1.91s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 26/782 [00:47<23:17,  1.85s/it]\u001b[A\n",
            "Computing embeddings:   3%|▎         | 27/782 [00:49<24:24,  1.94s/it]\u001b[A\n",
            "Computing embeddings:   4%|▎         | 28/782 [00:50<23:08,  1.84s/it]\u001b[A\n",
            "Computing embeddings:   4%|▎         | 29/782 [00:52<21:18,  1.70s/it]\u001b[A\n",
            "Computing embeddings:   4%|▍         | 30/782 [00:54<23:05,  1.84s/it]\u001b[A\n",
            "Computing embeddings:   4%|▍         | 31/782 [00:56<24:31,  1.96s/it]\u001b[A\n",
            "Computing embeddings:   4%|▍         | 32/782 [00:59<26:21,  2.11s/it]\u001b[A\n",
            "Computing embeddings:   4%|▍         | 33/782 [01:00<23:55,  1.92s/it]\u001b[A\n",
            "Computing embeddings:   4%|▍         | 34/782 [01:02<22:11,  1.78s/it]\u001b[A\n",
            "Computing embeddings:   4%|▍         | 35/782 [01:03<21:17,  1.71s/it]\u001b[A\n",
            "Computing embeddings:   5%|▍         | 36/782 [01:04<19:10,  1.54s/it]\u001b[A\n",
            "Computing embeddings:   5%|▍         | 37/782 [01:06<19:46,  1.59s/it]\u001b[A\n",
            "Computing embeddings:   5%|▍         | 38/782 [01:07<19:22,  1.56s/it]\u001b[A\n",
            "Computing embeddings:   5%|▍         | 39/782 [01:10<22:27,  1.81s/it]\u001b[A\n",
            "Computing embeddings:   5%|▌         | 40/782 [01:12<23:52,  1.93s/it]\u001b[A\n",
            "Computing embeddings:   5%|▌         | 41/782 [01:13<21:09,  1.71s/it]\u001b[A\n",
            "Computing embeddings:   5%|▌         | 42/782 [01:15<20:44,  1.68s/it]\u001b[A\n",
            "Computing embeddings:   5%|▌         | 43/782 [01:18<24:31,  1.99s/it]\u001b[A\n",
            "Computing embeddings:   6%|▌         | 44/782 [01:19<23:50,  1.94s/it]\u001b[A\n",
            "Computing embeddings:   6%|▌         | 45/782 [01:21<24:26,  1.99s/it]\u001b[A\n",
            "Computing embeddings:   6%|▌         | 46/782 [01:24<26:57,  2.20s/it]\u001b[A\n",
            "Computing embeddings:   6%|▌         | 47/782 [01:26<25:05,  2.05s/it]\u001b[A\n",
            "Computing embeddings:   6%|▌         | 48/782 [01:28<24:37,  2.01s/it]\u001b[A\n",
            "Computing embeddings:   6%|▋         | 49/782 [01:29<23:06,  1.89s/it]\u001b[A\n",
            "Computing embeddings:   6%|▋         | 50/782 [01:31<22:29,  1.84s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 51/782 [01:33<21:04,  1.73s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 52/782 [01:34<21:00,  1.73s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 53/782 [01:37<23:53,  1.97s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 54/782 [01:39<23:41,  1.95s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 55/782 [01:41<24:32,  2.03s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 56/782 [01:43<23:45,  1.96s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 57/782 [01:45<24:19,  2.01s/it]\u001b[A\n",
            "Computing embeddings:   7%|▋         | 58/782 [01:47<23:36,  1.96s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 59/782 [01:50<27:09,  2.25s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 60/782 [01:52<26:46,  2.23s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 61/782 [01:54<25:36,  2.13s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 62/782 [01:56<25:35,  2.13s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 63/782 [01:58<26:44,  2.23s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 64/782 [02:00<26:01,  2.18s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 65/782 [02:03<27:12,  2.28s/it]\u001b[A\n",
            "Computing embeddings:   8%|▊         | 66/782 [02:06<29:50,  2.50s/it]\u001b[A\n",
            "Computing embeddings:   9%|▊         | 67/782 [02:07<25:55,  2.18s/it]\u001b[A\n",
            "Computing embeddings:   9%|▊         | 68/782 [02:10<26:17,  2.21s/it]\u001b[A\n",
            "Computing embeddings:   9%|▉         | 69/782 [02:12<25:52,  2.18s/it]\u001b[A\n",
            "Computing embeddings:   9%|▉         | 70/782 [02:14<25:08,  2.12s/it]\u001b[A\n",
            "Computing embeddings:   9%|▉         | 71/782 [02:16<25:00,  2.11s/it]\u001b[A\n",
            "Computing embeddings:   9%|▉         | 72/782 [02:18<26:45,  2.26s/it]\u001b[A\n",
            "Computing embeddings:   9%|▉         | 73/782 [02:20<25:05,  2.12s/it]\u001b[A\n",
            "Computing embeddings:   9%|▉         | 74/782 [02:22<23:18,  1.98s/it]\u001b[A\n",
            "Computing embeddings:  10%|▉         | 75/782 [02:24<23:22,  1.98s/it]\u001b[A\n",
            "Computing embeddings:  10%|▉         | 76/782 [02:26<23:27,  1.99s/it]\u001b[A\n",
            "Computing embeddings:  10%|▉         | 77/782 [02:28<25:32,  2.17s/it]\u001b[A\n",
            "Computing embeddings:  10%|▉         | 78/782 [02:31<28:26,  2.42s/it]\u001b[A\n",
            "Computing embeddings:  10%|█         | 79/782 [02:33<26:11,  2.24s/it]\u001b[A\n",
            "Computing embeddings:  10%|█         | 80/782 [02:35<24:59,  2.14s/it]\u001b[A\n",
            "Computing embeddings:  10%|█         | 81/782 [02:37<24:30,  2.10s/it]\u001b[A\n",
            "Computing embeddings:  10%|█         | 82/782 [02:40<25:16,  2.17s/it]\u001b[A\n",
            "Computing embeddings:  11%|█         | 83/782 [02:42<26:04,  2.24s/it]\u001b[A\n",
            "Computing embeddings:  11%|█         | 84/782 [02:45<28:46,  2.47s/it]\u001b[A\n",
            "Computing embeddings:  11%|█         | 85/782 [02:47<27:24,  2.36s/it]\u001b[A\n",
            "Computing embeddings:  11%|█         | 86/782 [02:49<25:20,  2.18s/it]\u001b[A\n",
            "Computing embeddings:  11%|█         | 87/782 [02:51<25:37,  2.21s/it]\u001b[A\n",
            "Computing embeddings:  11%|█▏        | 88/782 [03:07<1:11:51,  6.21s/it]\u001b[A\n",
            "Computing embeddings:  11%|█▏        | 89/782 [03:29<2:06:12, 10.93s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 90/782 [03:50<2:42:04, 14.05s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 91/782 [04:12<3:10:21, 16.53s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 92/782 [04:35<3:32:35, 18.49s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 93/782 [04:56<3:40:37, 19.21s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 94/782 [05:18<3:48:01, 19.89s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 95/782 [05:42<4:02:49, 21.21s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 96/782 [06:04<4:05:44, 21.49s/it]\u001b[A\n",
            "Computing embeddings:  12%|█▏        | 97/782 [06:26<4:05:24, 21.50s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 98/782 [06:49<4:10:35, 21.98s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 99/782 [07:10<4:08:42, 21.85s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 100/782 [07:33<4:10:19, 22.02s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 101/782 [07:55<4:12:16, 22.23s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 102/782 [08:17<4:11:04, 22.15s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 103/782 [08:40<4:12:48, 22.34s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 104/782 [09:03<4:12:59, 22.39s/it]\u001b[A\n",
            "Computing embeddings:  13%|█▎        | 105/782 [09:24<4:08:25, 22.02s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▎        | 106/782 [09:44<4:03:14, 21.59s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▎        | 107/782 [10:05<3:59:45, 21.31s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▍        | 108/782 [10:27<4:00:04, 21.37s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▍        | 109/782 [10:49<4:04:02, 21.76s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▍        | 110/782 [11:11<4:05:01, 21.88s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▍        | 111/782 [11:33<4:04:33, 21.87s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▍        | 112/782 [11:55<4:04:13, 21.87s/it]\u001b[A\n",
            "Computing embeddings:  14%|█▍        | 113/782 [12:17<4:04:06, 21.89s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▍        | 114/782 [12:38<4:01:26, 21.69s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▍        | 115/782 [12:59<3:59:21, 21.53s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▍        | 116/782 [13:21<3:58:42, 21.51s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▍        | 117/782 [13:41<3:54:49, 21.19s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▌        | 118/782 [14:07<4:10:34, 22.64s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▌        | 119/782 [14:29<4:05:52, 22.25s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▌        | 120/782 [14:50<4:03:33, 22.07s/it]\u001b[A\n",
            "Computing embeddings:  15%|█▌        | 121/782 [15:12<4:00:48, 21.86s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▌        | 122/782 [15:33<3:58:42, 21.70s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▌        | 123/782 [15:54<3:55:03, 21.40s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▌        | 124/782 [16:15<3:54:55, 21.42s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▌        | 125/782 [16:36<3:53:09, 21.29s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▌        | 126/782 [16:58<3:53:20, 21.34s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▌        | 127/782 [17:20<3:56:02, 21.62s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▋        | 128/782 [17:41<3:53:50, 21.45s/it]\u001b[A\n",
            "Computing embeddings:  16%|█▋        | 129/782 [18:02<3:53:31, 21.46s/it]\u001b[A\n",
            "Computing embeddings:  17%|█▋        | 130/782 [18:24<3:54:46, 21.60s/it]\u001b[A\n",
            "Computing embeddings:  17%|█▋        | 131/782 [18:47<3:56:14, 21.77s/it]\u001b[A\n",
            "Computing embeddings:  17%|█▋        | 132/782 [19:08<3:56:21, 21.82s/it]\u001b[A\n",
            "Computing embeddings:  17%|█▋        | 133/782 [19:29<3:52:22, 21.48s/it]\u001b[A\n",
            "Computing embeddings:  17%|█▋        | 134/782 [19:51<3:51:49, 21.47s/it]\u001b[A\n",
            "Computing embeddings:  17%|█▋        | 135/782 [20:20<4:16:42, 23.81s/it]\u001b[A\n",
            "Computing embeddings:  17%|█▋        | 136/782 [20:40<4:03:01, 22.57s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 137/782 [21:02<4:01:39, 22.48s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 138/782 [21:24<3:59:48, 22.34s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 139/782 [21:45<3:54:47, 21.91s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 140/782 [22:07<3:56:50, 22.14s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 141/782 [22:31<4:00:08, 22.48s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 142/782 [22:52<3:57:37, 22.28s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 143/782 [23:15<3:56:52, 22.24s/it]\u001b[A\n",
            "Computing embeddings:  18%|█▊        | 144/782 [23:36<3:54:16, 22.03s/it]\u001b[A\n",
            "Computing embeddings:  19%|█▊        | 145/782 [23:58<3:53:35, 22.00s/it]\u001b[A\n",
            "Computing embeddings:  19%|█▊        | 146/782 [24:20<3:53:17, 22.01s/it]\u001b[A\n",
            "Computing embeddings:  19%|█▉        | 147/782 [24:41<3:50:40, 21.80s/it]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gated_fuse_embeddings(\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    out_dir: str = 'outputs',\n",
        "    gate_type: str = 'scalar'  # 'scalar' or 'vector'\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Fuse text and image embeddings using gated fusion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_embeddings : np.ndarray of shape (N, D_text)\n",
        "    image_embeddings : np.ndarray of shape (N, D_img)\n",
        "    out_dir : str\n",
        "        Directory to save fused embeddings.\n",
        "    gate_type : str\n",
        "        'scalar': same gate for all dimensions\n",
        "        'vector': different gate per dimension (requires D_text == D_img)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fused : np.ndarray\n",
        "        Fused embeddings\n",
        "    \"\"\"\n",
        "    if text_embeddings.shape[0] != image_embeddings.shape[0]:\n",
        "        raise ValueError('Text and Image embeddings must have same number of rows')\n",
        "\n",
        "    if gate_type == 'scalar':\n",
        "        # learn a single scalar gate (0 <= alpha <= 1) for the entire embedding\n",
        "        alpha = 0.5  # you can tune this, or make it learnable later\n",
        "        fused = alpha * text_embeddings + (1 - alpha) * image_embeddings\n",
        "    elif gate_type == 'vector':\n",
        "        if text_embeddings.shape[1] != image_embeddings.shape[1]:\n",
        "            raise ValueError('For vector gating, text and image dims must match')\n",
        "        alpha_vec = 0.5  # same for all samples; could also be array of shape (D,)\n",
        "        fused = alpha_vec * text_embeddings + (1 - alpha_vec) * image_embeddings\n",
        "    else:\n",
        "        raise ValueError(\"gate_type must be 'scalar' or 'vector'\")\n",
        "\n",
        "    # Save\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    np.save(os.path.join(out_dir, 'fused_x.npy'), fused)\n",
        "    print(f\"Saved fused_x.npy with shape {fused.shape}\")\n",
        "    return fused"
      ],
      "metadata": {
        "id": "Bx54tFmfdoZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fused = gated_fuse_embeddings(text_embeddings=text_embs, image_embeddings=img_embs, out_dir='outputs', gate_type='scalar')"
      ],
      "metadata": {
        "id": "n650BQySdocg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hczhXoX0dofW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CivZ7wudoi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def robust_compute_embeddings(df: pd.DataFrame,\n",
        "                              images_dir: str,\n",
        "                              model_name: str = 'openai/clip-vit-base-patch32',\n",
        "                              batch_size: int = 32,\n",
        "                              device: str = None,\n",
        "                              out_dir: str = 'outputs'):\n",
        "    \"\"\"Robustly compute text and image embeddings for all rows, falling back to single-row processing on failure.\"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = CLIPModel.from_pretrained(model_name)\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    def load_image_safe(path: str, resize_to=(224,224)):\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img = img.resize(resize_to, resample=Image.BICUBIC)\n",
        "            return img\n",
        "        except Exception:\n",
        "            return Image.new('RGB', resize_to, (0,0,0))\n",
        "\n",
        "    n = len(df)\n",
        "    text_embs = []\n",
        "    image_embs = []\n",
        "    idx = 0\n",
        "\n",
        "    while idx < n:\n",
        "        end = min(n, idx + batch_size)\n",
        "        batch = df.iloc[idx:end]\n",
        "\n",
        "        texts = batch['catalog_content_clean'].fillna('').tolist()\n",
        "        images = [ load_image_safe(os.path.join(images_dir, str(fn))) for fn in batch['image_name'].tolist() ]\n",
        "\n",
        "        try:\n",
        "            inputs = processor(text=texts, images=images, return_tensors='pt', padding=True, truncation=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            t_emb = outputs.text_embeds.detach().cpu().numpy()\n",
        "            i_emb = outputs.image_embeds.detach().cpu().numpy()\n",
        "\n",
        "            # ensure batch output count matches expectation\n",
        "            if t_emb.shape[0] != (end - idx) or i_emb.shape[0] != (end - idx):\n",
        "                raise RuntimeError(\"batch output length mismatch\")\n",
        "\n",
        "            # L2 normalize\n",
        "            t_emb = t_emb / (np.linalg.norm(t_emb, axis=1, keepdims=True) + 1e-12)\n",
        "            i_emb = i_emb / (np.linalg.norm(i_emb, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "            text_embs.append(t_emb)\n",
        "            image_embs.append(i_emb)\n",
        "            print(f\"Processed rows {idx}..{end-1}\")\n",
        "            idx = end\n",
        "\n",
        "        except Exception as e:\n",
        "            # fallback: process items one-by-one for this batch\n",
        "            print(f\"Batch failed at rows {idx}..{end-1} with error: {e}. Falling back to single-item processing.\")\n",
        "            for j in range(idx, end):\n",
        "                row = df.iloc[[j]]\n",
        "                t = row['catalog_content_clean'].fillna('').tolist()\n",
        "                p = os.path.join(images_dir, str(row['image_name'].values[0]))\n",
        "                im = load_image_safe(p)\n",
        "                inputs = processor(text=t, images=[im], return_tensors='pt', padding=True, truncation=True)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    out = model(**inputs)\n",
        "                t_emb = out.text_embeds.detach().cpu().numpy()\n",
        "                i_emb = out.image_embeds.detach().cpu().numpy()\n",
        "                t_emb = t_emb / (np.linalg.norm(t_emb, axis=1, keepdims=True) + 1e-12)\n",
        "                i_emb = i_emb / (np.linalg.norm(i_emb, axis=1, keepdims=True) + 1e-12)\n",
        "                text_embs.append(t_emb)\n",
        "                image_embs.append(i_emb)\n",
        "                print(f\"Processed single row {j}\")\n",
        "            idx = end\n",
        "\n",
        "    text_embeddings = np.vstack(text_embs)\n",
        "    image_embeddings = np.vstack(image_embs)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    np.save(os.path.join(out_dir, 'text_embeddings.npy'), text_embeddings)\n",
        "    np.save(os.path.join(out_dir, 'image_embeddings.npy'), image_embeddings)\n",
        "    print(f\"Saved embeddings. shapes: {text_embeddings.shape}, {image_embeddings.shape}\")\n",
        "    return text_embeddings, image_embeddings"
      ],
      "metadata": {
        "id": "nfChbi3sy9I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fused = gated_fuse_embeddings(text_embeddings=text_emb, image_embeddings=img_emb, out_dir='outputs', gate_type='scalar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gozoRotGyH_5",
        "outputId": "c9792a55-c819-4903-f921-1ae41378a7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fused_x.npy with shape (50000, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # sanity-check shapes before training\n",
        "print('Shapes -> fused:', fused.shape, 'y:', y.shape)\n",
        "assert fused.shape[0] == y.shape[0], 'Row count mismatch between fused features and target y. Aborting.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fpza1Ch7Nqz",
        "outputId": "3a9cd874-9f7b-4139-a22d-7301053cc2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes -> fused: (50000, 512) y: (50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train and evaluate\n",
        "metrics = train_and_evaluate(fused, y, out_dir=out_dir, seed=seed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMHlw93X7oey",
        "outputId": "585c57ba-5c73-40df-fd04-55f119a145a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.35368328\n",
            "Validation score: 0.308386\n",
            "Iteration 2, loss = 0.26331708\n",
            "Validation score: 0.337818\n",
            "Iteration 3, loss = 0.21889687\n",
            "Validation score: 0.333717\n",
            "Iteration 4, loss = 0.18110540\n",
            "Validation score: 0.317339\n",
            "Iteration 5, loss = 0.14854704\n",
            "Validation score: 0.306425\n",
            "Iteration 6, loss = 0.11550208\n",
            "Validation score: 0.297766\n",
            "Iteration 7, loss = 0.09315878\n",
            "Validation score: 0.298476\n",
            "Iteration 8, loss = 0.07429024\n",
            "Validation score: 0.279432\n",
            "Iteration 9, loss = 0.06067729\n",
            "Validation score: 0.278972\n",
            "Iteration 10, loss = 0.05301992\n",
            "Validation score: 0.279647\n",
            "Iteration 11, loss = 0.04624707\n",
            "Validation score: 0.274169\n",
            "Iteration 12, loss = 0.04275590\n",
            "Validation score: 0.279820\n",
            "Iteration 13, loss = 0.03814903\n",
            "Validation score: 0.286585\n",
            "Iteration 14, loss = 0.03432167\n",
            "Validation score: 0.290033\n",
            "Iteration 15, loss = 0.03224676\n",
            "Validation score: 0.280038\n",
            "Iteration 16, loss = 0.03174972\n",
            "Validation score: 0.291243\n",
            "Iteration 17, loss = 0.03151827\n",
            "Validation score: 0.280407\n",
            "Iteration 18, loss = 0.03105877\n",
            "Validation score: 0.279512\n",
            "Iteration 19, loss = 0.02964981\n",
            "Validation score: 0.275096\n",
            "Iteration 20, loss = 0.02849965\n",
            "Validation score: 0.284438\n",
            "Iteration 21, loss = 0.02755959\n",
            "Validation score: 0.285696\n",
            "Iteration 22, loss = 0.02703339\n",
            "Validation score: 0.295356\n",
            "Iteration 23, loss = 0.02633313\n",
            "Validation score: 0.261899\n",
            "Iteration 24, loss = 0.02647262\n",
            "Validation score: 0.284390\n",
            "Iteration 25, loss = 0.02595327\n",
            "Validation score: 0.292911\n",
            "Iteration 26, loss = 0.02556517\n",
            "Validation score: 0.270516\n",
            "Iteration 27, loss = 0.02562391\n",
            "Validation score: 0.287876\n",
            "Iteration 28, loss = 0.02454190\n",
            "Validation score: 0.303820\n",
            "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
            "\n",
            "Evaluation on test set:\n",
            "MAE: 0.592566\n",
            "RMSE: 0.755101\n",
            "R2: 0.354190\n",
            "SMAPE: 23.15%\n",
            "Saved model and scaler to outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df shape:\", df.shape)                     # should be (1000, ...)\n",
        "import numpy as np\n",
        "try:\n",
        "    fused = np.load('outputs/fused_x.npy')       # if you previously saved fused\n",
        "    print(\"fused shape (loaded):\", fused.shape)\n",
        "except Exception as e:\n",
        "    print(\"No fused_x.npy found or failed to load:\", e)\n",
        "\n",
        "try:\n",
        "    text_emb = np.load('outputs/text_embeddings.npy')\n",
        "    img_emb = np.load('outputs/image_embeddings.npy')\n",
        "    print(\"text_emb shape:\", text_emb.shape)\n",
        "    print(\"image_emb shape:\", img_emb.shape)\n",
        "except Exception as e:\n",
        "    print(\"Failed to load embeddings:\", e)\n",
        "\n",
        "print(\"y shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-o-hGFt4s2b",
        "outputId": "82a3c070-a4d1-42c1-be87-3a2e31b0fb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df shape: (50000, 5)\n",
            "fused shape (loaded): (50000, 512)\n",
            "text_emb shape: (50000, 512)\n",
            "image_emb shape: (50000, 512)\n",
            "y shape: (50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BJ4jW9C_7dGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}